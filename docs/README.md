# Implicit Neural Representations for Remote Sensing Scene

<!-- ABOUT THE PROJECT -->
## About The Project
(https://shadowxzt.github.io/Implicit-remote-sensing-scene.github.io/)

Remote sensing imagery, captured from top-view, can cover a large range of visual content. However, compared with ground-view data, they usually lack some informative details of the scene. In this context, recent progress on neural rendering and implicit neural representation makes the photorealistic cross-view synthesis possible by predicting the ground-view image given the top-view data. Cross-view synthesis enables the scene understanding from remote sensing and street views. Many applications can benefit from this task, including autonomous driving and navigation. Moreover, view synthesis between ground-view to top-view also bridges the gap between computer vision and remote sensing community and extends the application areas of remote sensing methods.

<p align="right">(<a href="#top">back to top</a>)</p>


<div  align="center">    
 <img src="img/samples.png" width = "1200" height = "400" alt="example" align=center />
</div>

### Prerequisites

1. First, run the following script to connect the the annotation server
  ```sh
  Windows PowerShell
  ```
  
2. Thenï¼Œ run the following script in the powershell terminal:
 ```sh
 ssh -L localhost:8002:172.17.0.3:8000 xshadow@AI4EO2.sipeo.lrg.tum.de
 ```
3. Finally, open a browser and use an account to log into the annotation system:
 
   [localhost:8002](http://localhost:8002)
 
  The following accounts are avaliable now:
  ```python
  username: user1  passwd: user1
  username: user2  passwd: user2
  ```
  
### Annotation
1. Annotate 9 pairs of corresponding points
2. Using the following link to view and select image samples for annotation:

   [localhost:8002/table](http://localhost:8000/table)

3. Using the following link to view the summary of the annotation process:

   [localhost:8002/summary](http://localhost:8000/summary)


<p align="right">(<a href="#top">back to top</a>)</p>


<!-- CONTACT -->
## Contact

Zhitong Xiong - zhitong.xiong@tum.de

Project Link: [Implicit Neural Representations for Remote Sensing Scene]([https://github.com/your_username/repo_name](https://shadowxzt.github.io/Implicit-remote-sensing-scene.github.io/))

<p align="right">(<a href="#top">back to top</a>)</p>



## Problem statement:
Assume that we have got a 3D city model, which can be reconstructed by MVS from multiple satellite images. Given the 3D model and street-view images at several locations, we aim to render novel views at other locations continuously.

## Data description:
*	Top view satellite image. The red dot indicates the location for capturing the street view images.
*	The corresponding 3D city model (mesh and texture), with pixel-wise segmentation annotations. Georeferenced to the top-view image and street view images.
*	The corresponding street view images at coordinate (6063,4597) of the 3D model. The filenames of these images indicate the camera poses (location, pitch, fov and heading). 
*	The corresponding 360 street view image
*	Illustration of an initial idea on novel street view synthesis.

In Data_sample/panos.txt:

        Pano_ID,              Lat,               Lon,          Y in 3D model,     X in 3D model
  
        AXD-LhS8HF3v1XajBnV2nA, 60.1644797333783, 24.92898345052589, 4400.232103129849,  6056.963851299137

Pano_ID can be used to download the street view image;
Lat and Lon can be used to download the Street view image or the top-view satellite image
X and Y can be used to locate in the 3D mesh model and render the synthetic image

Mesh_render.py is a sample code for rendering the synthetic image. Using the street view API, we can get the corresponding real image.


